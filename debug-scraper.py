#!/usr/bin/env python3
"""
Debug script to test the scraper independently.
Usage: python debug-scraper.py
"""

import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from scraper import KaktusScraper
from database import DatabaseManager
from config import Config
import logging

# Set up detailed logging
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)

def main():
    print("ğŸ” Kaktus Scraper Debug Tool")
    print("=" * 50)
    
    # Load configuration
    Config.setup_logging()
    
    # Create a mock database manager (won't actually save to DB)
    class MockDatabaseManager:
        def is_post_processed(self, post_hash):
            return False
        
        def add_processed_post(self, post_hash, title, content, event_datetime=None):
            print(f"âœ… Would save post: {title}")
            return True
    
    mock_db = MockDatabaseManager()
    
    # Create scraper
    scraper = KaktusScraper(Config.SCRAPE_URL, mock_db, 60)
    
    print(f"ğŸ“¡ Fetching: {Config.SCRAPE_URL}")
    print("-" * 50)
    
    # Fetch and parse the page
    soup = scraper.fetch_page()
    if not soup:
        print("âŒ Failed to fetch webpage")
        return
    
    print("âœ… Page fetched successfully")
    print(f"ğŸ“„ Page title: {soup.title.string if soup.title else 'No title'}")
    print(f"ğŸ“Š Total elements: {len(soup.find_all())}")
    
    print("\n" + "=" * 50)
    print("ğŸ” Extracting post content...")
    print("-" * 50)
    
    # Extract post
    post_data = scraper.extract_latest_post(soup)
    
    if post_data:
        print("âœ… Post extracted successfully!")
        print(f"ğŸ“ Title: {post_data['title']}")
        print(f"ğŸ“… Event datetime: {post_data['event_datetime']}")
        print(f"ğŸ“„ Content: {post_data['content'][:200]}...")
        print(f"ğŸ”’ Hash: {post_data['post_hash'][:16]}...")
    else:
        print("âŒ No post data extracted")
        
        # Show some debug info
        page_text = soup.get_text()
        print(f"\nğŸ“ Page text sample (first 300 chars):")
        print("-" * 30)
        print(page_text[:300])
        print("-" * 30)
        
        # Look for specific keywords
        keywords = ['dobÃ­jeÄka', 'akce', 'bonus', 'navÃ­c', 'kredit', 'kaktus']
        print(f"\nğŸ” Keyword analysis:")
        for keyword in keywords:
            count = page_text.lower().count(keyword.lower())
            print(f"  {keyword}: {count} occurrences")
    
    print("\n" + "=" * 50)
    print("âœ… Debug complete!")

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"âŒ Debug script failed: {e}")
        import traceback
        traceback.print_exc()